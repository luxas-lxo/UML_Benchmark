# UML_Benchmark

A tool for evaluating UML class diagrams, inspired by academic work on automated model comparison and grading.

## Table of Contents
- [Description](#description)
- [Setup](#setup)
- [Author and References](#author-and-references)

## Description

**Work in progress**

This project implements an evaluation pipeline for automatically grading UML class diagrams generated by Large Language Models (LLMs). The workflow consists of the following steps:

### 1. Reference Solution Preparation

1.1 Read and extract PDF scans of the reference solution  
1.2 Translate all content into English  
1.3 Process the solution:
- 1.3.1 Extract the domain description
- 1.3.2 Extract the grading criteria
- 1.3.3 Create the solution class diagram (in PlantUML or as a Python structure)

### 2. LLM-Based Response Generation

2.1 Define a system prompt tailored to PlantUML output  
2.2 Construct the user prompt:
- 2.2.1 Add a clear, concise task instruction
- 2.2.2 Include the extracted domain description (from 1.3.1)  
2.3 Combine system and user prompt and send the request to an LLM (any model supported)  
2.4 Collect all generated LLM responses along with the reference diagram into a `.jsonl` file

### 3. Evaluation with UML_Benchmark

3.1 Initialize the Instructor Model using the reference diagram (1.3.3)  
3.2 Initialize the Grade Model using the instructor model and grading criteria (1.3.2)  
3.3 Initialize Student Models based on the LLM responses (from 2.4)  
3.4 For each Student Model:
- Match classes, attributes, operations, enums, and relationships
- Evaluate the quality of matches using the Grade Model

### 4. Save Results

The following outputs are stored in `.jsonl` format:
- LLM response
- Reference solution
- Grading score


## Setup

Install the required Python packages and necessary language resources:

```powershell
pip install -r requirements.txt
python -m spacy download en_core_web_lg
python -c "import nltk; nltk.download('wordnet'); nltk.download('wordnet_ic')"
```

## Author and References

**Author:** Lukas Leopold – [@luxas-lxo](https://github.com/luxas-lxo)

### Reference

This project is based on the following publication:

> Bian, W., Alam, O., & Kienzle, J. (2019). *Automated Grading of Class Diagrams*. In Proceedings of the ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C), pp. 700–709. https://doi.org/10.1109/MODELS-C.2019.00106

If you use this project in academic work, please cite the original paper:

```
@INPROCEEDINGS{8904595,
  author={Bian, Weiyi and Alam, Omar and Kienzle, Jörg},
  booktitle={2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)}, 
  title={Automated Grading of Class Diagrams}, 
  year={2019},
  volume={},
  number={},
  pages={700-709},
  keywords={automated grading;class diagrams;model comparison},
  doi={10.1109/MODELS-C.2019.00106}
}

```
